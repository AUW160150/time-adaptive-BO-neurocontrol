"""
Analytical sketches for Time-Adaptive BO (consistent with paper/assignment)
- Fig 1: System + forgetting kernel (weights = exp(-ε|t-t'|)), t_half = ln(2)/ε
- Fig 2: Cumulative regret with fixed drift β=0.02 using heuristic forms:
         R_std(t)=sqrt(t*log(t+1)) + β t^2
         R_adapt(t)=sqrt(t*log(t+1)) + β t
         R_oracle(t)=sqrt(t*log(t+1))
- Fig 3: Sensitivity of cumulative regret at T=50 vs ε for β∈{0.01,0.02,0.05}
         Under-forgetting (ε<<β): quadratic drift cost
         Near ε≈β: linear drift cost
         Over-forgetting (ε>>β): still linear in T but worse constant
All figures are labeled **(analytical sketch)** to avoid implying simulations.
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Rectangle

# ---------- style ----------
plt.rcParams.update({
    "font.size": 10,
    "font.family": "serif",
    "axes.labelsize": 10,
    "axes.titlesize": 11,
    "legend.fontsize": 9,
    "figure.dpi": 300
})

# ======================================================================
# FIGURE 1: System architecture + forgetting kernel weights (sketch)
# ======================================================================
fig1 = plt.figure(figsize=(12, 3.5))

# (A) Neural system panel
ax1 = plt.subplot(1, 3, 1)
ax1.axis("off"); ax1.set_xlim(0, 10); ax1.set_ylim(0, 10)

brain_box = FancyBboxPatch((2, 4), 6, 3, boxstyle="round,pad=0.1",
                           edgecolor="black", facecolor="lightgray", linewidth=2)
ax1.add_patch(brain_box)
ax1.text(5, 5.5, "Brain/Neural\nSystem", ha="center", va="center",
         fontsize=11, weight="bold")
ax1.text(5, 3.5, "Adapts over time", ha="center", va="center",
         fontsize=8, style="italic")

ax1.add_patch(FancyArrowPatch((0.5, 5.5), (1.8, 5.5), arrowstyle="->",
                              mutation_scale=20, linewidth=2, color="black"))
ax1.text(1.1, 6.5, "Stimulation\nparameters\nx = (I, f, τ)",
         ha="center", va="bottom", fontsize=9)

ax1.add_patch(FancyArrowPatch((8.2, 5.5), (9.5, 5.5), arrowstyle="->",
                              mutation_scale=20, linewidth=2, color="black"))
ax1.text(8.85, 6.5, "Response\nf(x,t)", ha="center", va="bottom", fontsize=9)

ax1.set_title("(A) Neural System", fontsize=11, weight="bold", loc="left")

# (B) BO loop panel
ax2 = plt.subplot(1, 3, 2)
ax2.axis("off"); ax2.set_xlim(0, 10); ax2.set_ylim(0, 10)

ax2.add_patch(Rectangle((1, 7), 8, 1.5, edgecolor="black", facecolor="white", linewidth=1.5))
ax2.text(5, 7.75, "Gaussian Process with\ntemporal kernel K((x,t), (x',t'))",
         ha="center", va="center", fontsize=8)

ax2.add_patch(FancyArrowPatch((5, 7), (5, 5.8), arrowstyle="->",
                              mutation_scale=15, linewidth=1.5, color="black"))
ax2.add_patch(Rectangle((1, 4.3), 8, 1.5, edgecolor="black", facecolor="white", linewidth=1.5))
ax2.text(5, 5.05, "Acquisition Function\nα(x,t) = μ(x,t) - κ·σ(x,t)",
         ha="center", va="center", fontsize=8)

ax2.add_patch(FancyArrowPatch((5, 4.3), (5, 3.1), arrowstyle="->",
                              mutation_scale=15, linewidth=1.5, color="black"))
ax2.add_patch(Rectangle((1, 1.6), 8, 1.5, edgecolor="black", facecolor="white", linewidth=1.5))
ax2.text(5, 2.35, "Select next sample\nx_{t+1}", ha="center", va="center", fontsize=8)

ax2.add_patch(FancyArrowPatch((9.2, 2.35), (9.5, 7.75), arrowstyle="->",
                              mutation_scale=15, linewidth=1.5, color="blue",
                              connectionstyle="arc3,rad=.5"))
ax2.text(9.8, 5, "Loop", ha="left", va="center", fontsize=8, color="blue", rotation=90)
ax2.set_title("(B) BO Loop", fontsize=11, weight="bold", loc="left")

# (C) Forgetting weights panel
ax3 = plt.subplot(1, 3, 3)
Δt = np.linspace(0, 20, 200)
for eps, col, lab in [(0, "black", "ε = 0 (no forgetting)"),
                      (0.1, "blue", "ε = 0.1"),
                      (0.5, "red", "ε = 0.5")]:
    w = np.ones_like(Δt) if eps == 0 else np.exp(-eps * Δt)
    ax3.plot(Δt, w, color=col, linewidth=2, label=lab)
ax3.set_xlabel("Time difference |t - t'|")
ax3.set_ylabel("Weight: exp(-ε |t - t'|)")
ax3.set_xlim(0, 20); ax3.set_ylim(0, 1.1); ax3.grid(True, alpha=0.3)
ax3.legend(loc="upper right", frameon=True)
ax3.set_title("(C) Forgetting Mechanism", fontsize=11, weight="bold", loc="left")

plt.tight_layout()
fig1.savefig("Figure1_Architecture.png", dpi=300, bbox_inches="tight")
fig1.savefig("Figure1_Architecture.pdf", bbox_inches="tight")

# ======================================================================
# FIGURE 2: Cumulative regret with fixed drift β=0.02 (heuristic)
# ======================================================================
fig2, ax = plt.subplots(figsize=(7, 5))

T = 50
t = np.arange(1, T + 1)
beta = 0.02  # fixed drift for this figure

# Heuristic scalings (match manuscript math)
regret_std    = np.sqrt(t * np.log(t + 1)) + beta * t**2
regret_adapt  = np.sqrt(t * np.log(t + 1)) + beta * t
regret_oracle = np.sqrt(t * np.log(t + 1))

ax.plot(t, regret_std,   "-",  color="black", linewidth=2.5, label="Standard BO")
ax.plot(t, regret_adapt, "--", color="blue",  linewidth=2.5, label="Time-aware BO (forgetting)")
ax.plot(t, regret_oracle,":",  color="red",   linewidth=2.5, label="Oracle (perfect tracking)")

ax.set_xlabel("Iteration (t)"); ax.set_ylabel("Cumulative regret R(T)")
ax.set_xlim(0, T); ax.set_ylim(0, 70)  # avoid clipping
ax.grid(True, alpha=0.3); ax.legend(loc="upper left", frameon=True, fontsize=10)
ax.set_title("Cumulative Regret with Drift β = 0.02  (analytical sketch)",
             fontsize=12, weight="bold", pad=10)

fig2.tight_layout()
fig2.savefig("Figure2_Regret.png", dpi=300, bbox_inches="tight")
fig2.savefig("Figure2_Regret.pdf", bbox_inches="tight")

# ======================================================================
# FIGURE 3: Sensitivity vs ε at T=50 for multiple β (heuristic)
# ======================================================================
fig3, ax = plt.subplots(figsize=(7, 5))
epsilon = np.linspace(0, 0.4, 400)
beta_values = [(0.01, "green"), (0.02, "blue"), (0.05, "red")]
T_val = 50

base_regret = np.sqrt(T_val * np.log(T_val))  # same base in all curves

for b, col in beta_values:
    # Penalty piecewise to illustrate regimes in the paper
    penalty = np.where(epsilon < 0.5 * b,
                       b * T_val**2 * (1 - 2*epsilon/b),            # under-forgetting → quadratic drift cost
                       np.where(epsilon > 2 * b,
                                b * T_val * (1 + (epsilon/b - 1)),   # over-forgetting → linear, worse constant
                                b * T_val))                          # near ε≈β → linear drift cost
    regret = base_regret + penalty
    ax.plot(epsilon, regret, color=col, linewidth=2.5,
            label=f"β = {b:.02f}")

    # Mark ε ≈ β
    eps_opt = b
    opt_regret = base_regret + b * T_val
    ax.plot([eps_opt], [opt_regret], "o", color=col, markersize=7,
            markeredgecolor="black", markeredgewidth=1.2)
    ax.axvline(eps_opt, color=col, linestyle="--", linewidth=1, alpha=0.6)

# Robust band for β=0.02
beta_mid = 0.02
ax.axvspan(0.5*beta_mid, 2*beta_mid, alpha=0.15, color="gray",
           label="Robust region (0.5β–2β) for β=0.02")

ax.set_xlabel("Forgetting factor ε"); ax.set_ylabel("Cumulative regret R(50)")
ax.set_xlim(0, 0.4); ax.set_ylim(0, 70); ax.grid(True, alpha=0.3)
ax.legend(loc="upper right", frameon=True, fontsize=9)
ax.set_title("Algorithm Performance Sensitivity to ε  (analytical sketch)",
             fontsize=12, weight="bold", pad=10)

fig3.tight_layout()
fig3.savefig("Figure3_Sensitivity.png", dpi=300, bbox_inches="tight")
fig3.savefig("Figure3_Sensitivity.pdf", bbox_inches="tight")

print("Saved:",
      "Figure1_Architecture.(png|pdf),",
      "Figure2_Regret.(png|pdf),",
      "Figure3_Sensitivity.(png|pdf)")
